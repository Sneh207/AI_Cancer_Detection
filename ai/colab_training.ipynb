{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Lung Cancer Detection - Google Colab Training\n",
    "\n",
    "This notebook trains the ResNet50 model on GPU with balanced sampling to handle class imbalance.\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Google Drive\n",
    "2. Upload your dataset and code\n",
    "3. Install dependencies\n",
    "4. Train the model with GPU acceleration\n",
    "5. Download the trained checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Project Files\n",
    "\n",
    "**Option A: Upload ZIP to Google Drive**\n",
    "1. Compress your project: `AI_Cancer_Detection/ai/` folder\n",
    "2. Upload to Google Drive: `My Drive/AI_Cancer_Detection/`\n",
    "3. Run the cell below\n",
    "\n",
    "**Option B: Clone from GitHub** (if you have a repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Extract from Google Drive\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Update this path to your ZIP file location in Google Drive\n",
    "zip_path = '/content/drive/MyDrive/AI_Cancer_Detection/ai.zip'\n",
    "extract_path = '/content/ai'\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    print(f\"‚úÖ Extracted to {extract_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå ZIP file not found at {zip_path}\")\n",
    "    print(\"Please upload your project ZIP to Google Drive first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Clone from GitHub (uncomment if using)\n",
    "# !git clone https://github.com/YOUR_USERNAME/AI_Cancer_Detection.git /content/ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify project structure\n",
    "!ls -la /content/ai/\n",
    "!ls -la /content/ai/src/\n",
    "!ls -la /content/ai/data/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q albumentations==1.3.1\n",
    "!pip install -q timm==0.9.12\n",
    "!pip install -q scikit-learn==1.3.2\n",
    "!pip install -q tensorboard==2.15.1\n",
    "!pip install -q pyyaml==6.0.1\n",
    "!pip install -q Pillow==10.1.0\n",
    "!pip install -q matplotlib==3.8.2\n",
    "!pip install -q seaborn==0.13.0\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "import os\n",
    "os.chdir('/content/ai')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify config file\n",
    "import yaml\n",
    "\n",
    "config_path = '/content/ai/configs/config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current Configuration:\")\n",
    "print(f\"  Model: {config['model']['architecture']}\")\n",
    "print(f\"  Batch size: {config['data']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['epochs']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Image size: {config['data']['image_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Update config for Colab GPU training\n",
    "# Increase batch size for GPU\n",
    "config['data']['batch_size'] = 32  # Increase from 16\n",
    "config['data']['num_workers'] = 2  # Colab has 2 CPU cores\n",
    "config['training']['epochs'] = 50  # Full training\n",
    "\n",
    "# Save updated config\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"‚úÖ Config updated for GPU training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset and labels\n",
    "import pandas as pd\n",
    "\n",
    "labels_file = '/content/ai/data/raw/ChestXray_Binary_Labels.csv'\n",
    "df = pd.read_csv(labels_file)\n",
    "\n",
    "print(f\"Total images: {len(df)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['BinaryLabel'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['BinaryLabel'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if images exist\n",
    "import os\n",
    "\n",
    "dataset_path = '/content/ai/data/raw/train_data/train'\n",
    "if os.path.exists(dataset_path):\n",
    "    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.png')]\n",
    "    print(f\"‚úÖ Found {len(image_files)} images in {dataset_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset path not found: {dataset_path}\")\n",
    "    print(\"Please ensure your images are uploaded to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Training with Balanced Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "!python /content/ai/main.py train \\\n",
    "    --config /content/ai/configs/config.yaml \\\n",
    "    --experiment-name colab_resnet50_balanced \\\n",
    "    --device cuda \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Find the latest experiment directory\n",
    "import os\n",
    "import glob\n",
    "\n",
    "log_dirs = glob.glob('/content/ai/experiments/*/logs')\n",
    "if log_dirs:\n",
    "    latest_log = max(log_dirs, key=os.path.getctime)\n",
    "    print(f\"Loading TensorBoard from: {latest_log}\")\n",
    "    %tensorboard --logdir {latest_log}\n",
    "else:\n",
    "    print(\"No log directories found yet. Training may not have started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest experiment\n",
    "import os\n",
    "import glob\n",
    "\n",
    "exp_dirs = glob.glob('/content/ai/experiments/colab_resnet50_balanced*')\n",
    "if exp_dirs:\n",
    "    latest_exp = max(exp_dirs, key=os.path.getctime)\n",
    "    print(f\"Latest experiment: {latest_exp}\")\n",
    "    \n",
    "    # Check for checkpoint\n",
    "    checkpoint_path = os.path.join(latest_exp, 'checkpoints', 'best_model.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        import torch\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        print(f\"\\n‚úÖ Best model checkpoint found!\")\n",
    "        print(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "        print(f\"Best AUC: {checkpoint.get('best_auc', 'N/A'):.4f}\")\n",
    "        print(f\"Checkpoint size: {os.path.getsize(checkpoint_path) / (1024**2):.1f} MB\")\n",
    "    else:\n",
    "        print(\"No checkpoint found yet\")\n",
    "else:\n",
    "    print(\"No experiment directories found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "import glob\n",
    "\n",
    "exp_dirs = glob.glob('/content/ai/experiments/colab_resnet50_balanced*')\n",
    "if exp_dirs:\n",
    "    latest_exp = max(exp_dirs, key=os.path.getctime)\n",
    "    checkpoint_path = os.path.join(latest_exp, 'checkpoints', 'best_model.pth')\n",
    "    \n",
    "    !python /content/ai/main.py evaluate \\\n",
    "        --config /content/ai/configs/config.yaml \\\n",
    "        --checkpoint {checkpoint_path} \\\n",
    "        --device cuda\n",
    "else:\n",
    "    print(\"No trained model found. Please complete training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoint to Google Drive for safekeeping\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "exp_dirs = glob.glob('/content/ai/experiments/colab_resnet50_balanced*')\n",
    "if exp_dirs:\n",
    "    latest_exp = max(exp_dirs, key=os.path.getctime)\n",
    "    checkpoint_path = os.path.join(latest_exp, 'checkpoints', 'best_model.pth')\n",
    "    \n",
    "    # Create destination in Google Drive\n",
    "    drive_dest = '/content/drive/MyDrive/AI_Cancer_Detection/trained_models/'\n",
    "    os.makedirs(drive_dest, exist_ok=True)\n",
    "    \n",
    "    # Copy checkpoint\n",
    "    dest_file = os.path.join(drive_dest, 'best_model_colab.pth')\n",
    "    shutil.copy2(checkpoint_path, dest_file)\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint saved to Google Drive: {dest_file}\")\n",
    "    print(f\"Size: {os.path.getsize(dest_file) / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Also copy the entire experiment folder\n",
    "    exp_name = os.path.basename(latest_exp)\n",
    "    exp_dest = os.path.join(drive_dest, exp_name)\n",
    "    shutil.copytree(latest_exp, exp_dest, dirs_exist_ok=True)\n",
    "    print(f\"‚úÖ Full experiment saved to: {exp_dest}\")\n",
    "else:\n",
    "    print(\"No experiment found to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checkpoint directly to your computer\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "exp_dirs = glob.glob('/content/ai/experiments/colab_resnet50_balanced*')\n",
    "if exp_dirs:\n",
    "    latest_exp = max(exp_dirs, key=os.path.getctime)\n",
    "    checkpoint_path = os.path.join(latest_exp, 'checkpoints', 'best_model.pth')\n",
    "    \n",
    "    print(f\"Downloading: {checkpoint_path}\")\n",
    "    files.download(checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "exp_dirs = glob.glob('/content/ai/experiments/colab_resnet50_balanced*')\n",
    "if exp_dirs:\n",
    "    latest_exp = max(exp_dirs, key=os.path.getctime)\n",
    "    results_dir = os.path.join(latest_exp, 'results')\n",
    "    \n",
    "    # Look for evaluation results\n",
    "    eval_files = glob.glob(os.path.join(results_dir, '**/evaluation_results.json'), recursive=True)\n",
    "    if eval_files:\n",
    "        with open(eval_files[0], 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        print(\"\\nüìä Model Performance:\")\n",
    "        print(f\"  AUC: {results.get('auc', 'N/A'):.4f}\")\n",
    "        print(f\"  Accuracy: {results.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  Precision: {results.get('precision', 'N/A'):.4f}\")\n",
    "        print(f\"  Recall: {results.get('recall', 'N/A'):.4f}\")\n",
    "        print(f\"  F1 Score: {results.get('f1', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Look for confusion matrix image\n",
    "    cm_files = glob.glob(os.path.join(results_dir, '**/confusion_matrix.png'), recursive=True)\n",
    "    if cm_files:\n",
    "        from IPython.display import Image, display\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        display(Image(filename=cm_files[0]))\n",
    "    \n",
    "    # Look for ROC curve\n",
    "    roc_files = glob.glob(os.path.join(results_dir, '**/roc_curve.png'), recursive=True)\n",
    "    if roc_files:\n",
    "        print(\"\\nROC Curve:\")\n",
    "        display(Image(filename=roc_files[0]))\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up space by removing temporary files\n",
    "# WARNING: Only run this after saving your model to Google Drive!\n",
    "\n",
    "# !rm -rf /content/ai/experiments/*/logs  # Remove TensorBoard logs\n",
    "# !rm -rf /content/ai/data/raw/train_data  # Remove dataset (keep in Drive)\n",
    "\n",
    "print(\"‚ö†Ô∏è Uncomment the lines above to clean up space\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
