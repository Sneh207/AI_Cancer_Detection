{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0225eb",
   "metadata": {},
   "source": [
    "# Baseline Model Development\n",
    "\n",
    "This notebook develops and trains baseline models for cancer detection from chest X-rays:\n",
    "- Custom CNN architecture\n",
    "- Transfer learning with pre-trained models\n",
    "- Model comparison and analysis\n",
    "- Performance evaluation\n",
    "\n",
    "**Authors:** Sneh Gupta and Arpit Bhardwaj  \n",
    "**Course:** CSET211 - Statistical Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_loader import DataManager, get_transforms\n",
    "from models import get_model, CustomCNN, ResNetModel\n",
    "from utils import seed_everything, print_device_info, AverageMeter\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "seed_everything(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74730b76",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fe494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for data loading\n",
    "config = {\n",
    "    'data': {\n",
    "        'dataset_path': '../data/raw/images',\n",
    "        'labels_file': '../data/raw/Data_Entry_2017_v2020.csv',\n",
    "        'image_size': 224,\n",
    "        'batch_size': 16,  # Smaller for notebook\n",
    "        'num_workers': 0,   # For Windows compatibility\n",
    "        'train_split': 0.7,\n",
    "        'val_split': 0.15,\n",
    "        'test_split': 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize data manager\n",
    "print(\"Initializing data manager...\")\n",
    "try:\n",
    "    data_manager = DataManager(config)\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, test_loader = data_manager.get_data_loaders()\n",
    "    \n",
    "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "    \n",
    "    # Calculate positive weight for class imbalance\n",
    "    pos_weight = data_manager.calculate_pos_weight()\n",
    "    print(f\"Positive weight for loss function: {pos_weight:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure the dataset is available in the correct directory.\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "    \n",
    "    from torch.utils.data import TensorDataset\n",
    "    \n",
    "    # Create dummy data\n",
    "    dummy_images = torch.randn(100, 3, 224, 224)\n",
    "    dummy_labels = torch.randint(0, 2, (100,)).float()\n",
    "    \n",
    "    dataset = TensorDataset(dummy_images, dummy_labels)\n",
    "    train_loader = DataLoader(dataset[:60], batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(dataset[60:80], batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(dataset[80:], batch_size=16, shuffle=False)\n",
    "    \n",
    "    pos_weight = 2.0\n",
    "    print(\"Using dummy data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba84b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of training data\n",
    "def visualize_batch(data_loader, title=\"Training Batch\"):\n",
    "    # Get one batch\n",
    "    images, labels = next(iter(data_loader))\n",
    "    \n",
    "    # Create subplot grid\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for idx in range(min(8, len(images))):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        \n",
    "        # Denormalize image for display\n",
    "        image = images[idx]\n",
    "        \n",
    "        # Handle different image formats\n",
    "        if image.shape[0] == 3:  # RGB\n",
    "            # Denormalize using ImageNet stats\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            image = image * std + mean\n",
    "            image = torch.clamp(image, 0, 1)\n",
    "            image = image.permute(1, 2, 0)\n",
    "        \n",
    "        axes[row, col].imshow(image, cmap='gray' if image.shape[-1] == 1 else None)\n",
    "        axes[row, col].set_title(f'Label: {\"Cancer\" if labels[idx] == 1 else \"Normal\"}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training batch\n",
    "if 'train_loader' in locals():\n",
    "    visualize_batch(train_loader, \"Training Data Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5c50c",
   "metadata": {},
   "source": [
    "## 2. Model Definition and Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a43145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "model_configs = {\n",
    "    'custom_cnn': {\n",
    "        'model': {\n",
    "            'architecture': 'custom_cnn',\n",
    "            'num_classes': 1,\n",
    "            'dropout': 0.5,\n",
    "            'pretrained': False\n",
    "        }\n",
    "    },\n",
    "    'resnet18': {\n",
    "        'model': {\n",
    "            'architecture': 'resnet18',\n",
    "            'num_classes': 1,\n",
    "            'dropout': 0.5,\n",
    "            'pretrained': True\n",
    "        }\n",
    "    },\n",
    "    'resnet50': {\n",
    "        'model': {\n",
    "            'architecture': 'resnet50',\n",
    "            'num_classes': 1,\n",
    "            'dropout': 0.5,\n",
    "            'pretrained': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create models and analyze their properties\n",
    "models = {}\n",
    "model_info = []\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    try:\n",
    "        model = get_model(config)\n",
    "        models[name] = model\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        model_info.append({\n",
    "            'Model': name,\n",
    "            'Total Parameters': f\"{total_params:,}\",\n",
    "            'Trainable Parameters': f\"{trainable_params:,}\",\n",
    "            'Memory (MB)': f\"{total_params * 4 / 1024 / 1024:.1f}\"  # Rough estimate\n",
    "        })\n",
    "        \n",
    "        print(f\"✓ {name} loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {name}: {e}\")\n",
    "\n",
    "# Display model comparison\n",
    "if model_info:\n",
    "    import pandas as pd\n",
    "    model_df = pd.DataFrame(model_info)\n",
    "    print(\"\\nModel Architecture Comparison:\")\n",
    "    print(model_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass for all models\n",
    "print(\"Testing forward pass for all models:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "        \n",
    "        print(f\"✓ {name}: Input {list(dummy_input.shape)} → Output {list(output.shape)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ {name}: Error in forward pass - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34802feb",
   "metadata": {},
   "source": [
    "## 3. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        \n",
    "        # Store predictions for metrics\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    train_acc = accuracy_score(all_targets, all_preds > 0.5)\n",
    "    train_auc = roc_auc_score(all_targets, all_preds) if len(np.unique(all_targets)) > 1 else 0.0\n",
    "    \n",
    "    return losses.avg, train_acc, train_auc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        \n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            \n",
    "            preds = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    val_acc = accuracy_score(all_targets, all_preds > 0.5)\n",
    "    val_precision = precision_score(all_targets, all_preds > 0.5, zero_division=0)\n",
    "    val_recall = recall_score(all_targets, all_preds > 0.5, zero_division=0)\n",
    "    val_f1 = f1_score(all_targets, all_preds > 0.5, zero_division=0)\n",
    "    val_auc = roc_auc_score(all_targets, all_preds) if len(np.unique(all_targets)) > 1 else 0.0\n",
    "    \n",
    "    return losses.avg, val_acc, val_precision, val_recall, val_f1, val_auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.001, pos_weight=1.0):\n",
    "    \"\"\"Complete training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function with class weighting\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_auc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_precision': [], \n",
    "        'val_recall': [], 'val_f1': [], 'val_auc': []\n",
    "    }\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc, train_auc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1, val_auc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train AUC: {train_auc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}')\n",
    "        print(f'Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            print(f'New best model! Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    return model, history, best_val_auc\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf27a81",
   "metadata": {},
   "source": [
    "## 4. Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6457a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Custom CNN as baseline\n",
    "if 'custom_cnn' in models:\n",
    "    print(\"Training Custom CNN (Baseline Model)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    custom_model = models['custom_cnn']\n",
    "    \n",
    "    # Train for fewer epochs in notebook\n",
    "    trained_custom, custom_history, custom_best_auc = train_model(\n",
    "        custom_model, train_loader, val_loader, \n",
    "        epochs=5, learning_rate=0.001, pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCustom CNN Training Complete!\")\n",
    "    print(f\"Best Validation AUC: {custom_best_auc:.4f}\")\n",
    "else:\n",
    "    print(\"Custom CNN model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet18 with transfer learning\n",
    "if 'resnet18' in models:\n",
    "    print(\"Training ResNet18 (Transfer Learning)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    resnet18_model = models['resnet18']\n",
    "    \n",
    "    # Train with lower learning rate for transfer learning\n",
    "    trained_resnet18, resnet18_history, resnet18_best_auc = train_model(\n",
    "        resnet18_model, train_loader, val_loader,\n",
    "        epochs=5, learning_rate=0.0001, pos_weight=pos_weight\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResNet18 Training Complete!\")\n",
    "    print(f\"Best Validation AUC: {resnet18_best_auc:.4f}\")\n",
    "else:\n",
    "    print(\"ResNet18 model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f0b29",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4532e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for all trained models\n",
    "def plot_training_history(histories, model_names):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['loss', 'acc', 'auc', 'precision', 'recall', 'f1']\n",
    "    metric_titles = ['Loss', 'Accuracy', 'AUC', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
    "        row, col = idx // 3, idx % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            if metric == 'loss':\n",
    "                ax.plot(history['train_loss'], f'{color}--', label=f'{name} Train', alpha=0.7)\n",
    "                ax.plot(history['val_loss'], color, label=f'{name} Val')\n",
    "            else:\n",
    "                val_key = f'val_{metric}'\n",
    "                if val_key in history:\n",
    "                    ax.plot(history[val_key], color, label=f'{name}')\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Collect histories from trained models\n",
    "trained_histories = []\n",
    "trained_names = []\n",
    "\n",
    "if 'custom_history' in locals():\n",
    "    trained_histories.append(custom_history)\n",
    "    trained_names.append('Custom CNN')\n",
    "\n",
    "if 'resnet18_history' in locals():\n",
    "    trained_histories.append(resnet18_history)\n",
    "    trained_names.append('ResNet18')\n",
    "\n",
    "if trained_histories:\n",
    "    plot_training_history(trained_histories, trained_names)\n",
    "else:\n",
    "    print(\"No training histories available to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison table\n",
    "comparison_data = []\n",
    "\n",
    "if 'custom_history' in locals():\n",
    "    custom_final = {\n",
    "        'Model': 'Custom CNN',\n",
    "        'Final Val Accuracy': f\"{custom_history['val_acc'][-1]:.4f}\",\n",
    "        'Final Val AUC': f\"{custom_history['val_auc'][-1]:.4f}\",\n",
    "        'Final Val Precision': f\"{custom_history['val_precision'][-1]:.4f}\",\n",
    "        'Final Val Recall': f\"{custom_history['val_recall'][-1]:.4f}\",\n",
    "        'Final Val F1': f\"{custom_history['val_f1'][-1]:.4f}\",\n",
    "        'Best Val AUC': f\"{custom_best_auc:.4f}\"\n",
    "    }\n",
    "    comparison_data.append(custom_final)\n",
    "\n",
    "if 'resnet18_history' in locals():\n",
    "    resnet18_final = {\n",
    "        'Model': 'ResNet18',\n",
    "        'Final Val Accuracy': f\"{resnet18_history['val_acc'][-1]:.4f}\",\n",
    "        'Final Val AUC': f\"{resnet18_history['val_auc'][-1]:.4f}\",\n",
    "        'Final Val Precision': f\"{resnet18_history['val_precision'][-1]:.4f}\",\n",
    "        'Final Val Recall': f\"{resnet18_history['val_recall'][-1]:.4f}\",\n",
    "        'Final Val F1': f\"{resnet18_history['val_f1'][-1]:.4f}\",\n",
    "        'Best Val AUC': f\"{resnet18_best_auc:.4f}\"\n",
    "    }\n",
    "    comparison_data.append(resnet18_final)\n",
    "\n",
    "if comparison_data:\n",
    "    import pandas as pd\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Determine best model\n",
    "    best_auc = max([float(row['Best Val AUC']) for row in comparison_data])\n",
    "    best_model = [row['Model'] for row in comparison_data if float(row['Best Val AUC']) == best_auc][0]\n",
    "    print(f\"\\nBest performing model: {best_model} (AUC: {best_auc:.4f})\")\n",
    "else:\n",
    "    print(\"No models trained for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93317ab7",
   "metadata": {},
   "source": [
    "## 6. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d339fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on test set\n",
    "def evaluate_on_test(model, test_loader, model_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=f'Testing {model_name}'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            \n",
    "            preds = torch.sigmoid(output).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = accuracy_score(all_targets, all_preds > 0.5)\n",
    "    test_precision = precision_score(all_targets, all_preds > 0.5, zero_division=0)\n",
    "    test_recall = recall_score(all_targets, all_preds > 0.5, zero_division=0)\n",
    "    test_f1 = f1_score(all_targets, all_preds > 0.5, zero_division=0)\n",
    "    test_auc = roc_auc_score(all_targets, all_preds) if len(np.unique(all_targets)) > 1 else 0.0\n",
    "    \n",
    "    print(f\"\\n{model_name} Test Results:\")\n",
    "    print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall:    {test_recall:.4f}\")\n",
    "    print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "    print(f\"AUC:       {test_auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'accuracy': test_acc,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1': test_f1,\n",
    "        'auc': test_auc\n",
    "    }\n",
    "\n",
    "# Test all trained models\n",
    "test_results = {}\n",
    "\n",
    "if 'trained_custom' in locals():\n",
    "    test_results['Custom CNN'] = evaluate_on_test(trained_custom, test_loader, 'Custom CNN')\n",
    "\n",
    "if 'trained_resnet18' in locals():\n",
    "    test_results['ResNet18'] = evaluate_on_test(trained_resnet18, test_loader, 'ResNet18')\n",
    "\n",
    "if test_results:\n",
    "    print(f\"\\nTested {len(test_results)} models on test set\")\n",
    "else:\n",
    "    print(\"No trained models available for testing\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
