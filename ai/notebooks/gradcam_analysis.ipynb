{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6467d5be",
   "metadata": {},
   "source": [
    "# Grad-CAM Visualization for Model Interpretability\n",
    "\n",
    "This notebook demonstrates explainable AI techniques for understanding what regions the trained models focus on when making cancer detection decisions:\n",
    "- Grad-CAM implementation and visualization\n",
    "- Attention analysis across different models\n",
    "- Clinical interpretation of model focus\n",
    "- Error case analysis with visualizations\n",
    "\n",
    "**Authors:** Sneh Gupta and Arpit Bhardwaj  \n",
    "**Course:** CSET211 - Statistical Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from gradcam import GradCAM, GradCAMVisualizer\n",
    "from models import get_model\n",
    "from data_loader import get_transforms\n",
    "from utils import load_config\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "CHECKPOINTS_DIR = '../experiments/checkpoints'\n",
    "SAMPLE_IMAGES_DIR = '../data/raw/images'\n",
    "RESULTS_DIR = '../experiments/gradcam_results'\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9488b4",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for model loading\n",
    "config = {\n",
    "    'model': {\n",
    "        'architecture': 'resnet50',  # Change this to match your trained model\n",
    "        'num_classes': 1,\n",
    "        'pretrained': True,\n",
    "        'dropout': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load model architecture\n",
    "model = get_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Try to load trained weights\n",
    "checkpoint_path = os.path.join(CHECKPOINTS_DIR, 'best_model.pth')\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"✓ Loaded trained model from {checkpoint_path}\")\n",
    "        print(f\"Model trained for {checkpoint.get('epoch', 'unknown')} epochs\")\n",
    "        print(f\"Best validation AUC: {checkpoint.get('val_auc', 'unknown'):.4f}\")\n",
    "        trained_model_available = True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading checkpoint: {e}\")\n",
    "        print(\"Using randomly initialized model for demonstration\")\n",
    "        model.eval()\n",
    "        trained_model_available = False\n",
    "else:\n",
    "    print(f\"✗ Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Using randomly initialized model for demonstration\")\n",
    "    model.eval()\n",
    "    trained_model_available = False\n",
    "\n",
    "print(f\"\\nModel architecture: {config['model']['architecture']}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e2b70",
   "metadata": {},
   "source": [
    "## 2. Grad-CAM Setup and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Grad-CAM visualizer\n",
    "try:\n",
    "    gradcam_viz = GradCAMVisualizer(model, device)\n",
    "    print(\"✓ Grad-CAM visualizer initialized successfully\")\n",
    "    \n",
    "    # Test with dummy input\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "    \n",
    "    print(f\"✓ Model forward pass successful\")\n",
    "    print(f\"Dummy input prediction: {prediction:.4f}\")\n",
    "    \n",
    "    # Test Grad-CAM generation\n",
    "    try:\n",
    "        heatmap = gradcam_viz.gradcam.generate_gradcam(dummy_input)\n",
    "        print(f\"✓ Grad-CAM heatmap generated: {heatmap.shape}\")\n",
    "        print(f\"Heatmap range: [{np.min(heatmap):.3f}, {np.max(heatmap):.3f}]\")\n",
    "        gradcam_working = True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Grad-CAM generation failed: {e}\")\n",
    "        gradcam_working = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error initializing Grad-CAM: {e}\")\n",
    "    gradcam_working = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082706c",
   "metadata": {},
   "source": [
    "## 3. Load Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sample images for visualization\n",
    "sample_images = []\n",
    "\n",
    "if os.path.exists(SAMPLE_IMAGES_DIR):\n",
    "    # Look for image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.dcm', '.dicom']\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        pattern = f\"*{ext}\"\n",
    "        found_images = list(Path(SAMPLE_IMAGES_DIR).glob(pattern))\n",
    "        sample_images.extend([str(p) for p in found_images[:5]])  # Limit to 5 per extension\n",
    "        \n",
    "        if len(sample_images) >= 10:  # Limit total samples\n",
    "            break\n",
    "    \n",
    "    sample_images = sample_images[:10]  # Final limit\n",
    "    print(f\"Found {len(sample_images)} sample images\")\n",
    "    \n",
    "    if sample_images:\n",
    "        for i, img_path in enumerate(sample_images[:5]):\n",
    "            print(f\"  {i+1}. {os.path.basename(img_path)}\")\n",
    "else:\n",
    "    print(f\"Sample images directory not found: {SAMPLE_IMAGES_DIR}\")\n",
    "    print(\"Creating synthetic sample images for demonstration...\")\n",
    "    \n",
    "    # Create synthetic chest X-ray-like images\n",
    "    sample_images = []\n",
    "    synthetic_dir = os.path.join(RESULTS_DIR, 'synthetic_samples')\n",
    "    os.makedirs(synthetic_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Create synthetic X-ray-like image\n",
    "        img = np.random.rand(256, 256) * 0.3 + 0.2  # Dark background\n",
    "        \n",
    "        # Add chest-like structures\n",
    "        center_x, center_y = 128, 128\n",
    "        y, x = np.ogrid[:256, :256]\n",
    "        \n",
    "        # Lung regions (brighter)\n",
    "        left_lung = ((x - 80)**2 + (y - center_y)**2) < 3000\n",
    "        right_lung = ((x - 176)**2 + (y - center_y)**2) < 3000\n",
    "        \n",
    "        img[left_lung] += 0.4\n",
    "        img[right_lung] += 0.4\n",
    "        \n",
    "        # Add some noise\n",
    "        img += np.random.normal(0, 0.1, img.shape)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Convert to PIL and save\n",
    "        img_pil = Image.fromarray((img * 255).astype(np.uint8), mode='L')\n",
    "        img_path = os.path.join(synthetic_dir, f'synthetic_xray_{i+1}.png')\n",
    "        img_pil.save(img_path)\n",
    "        sample_images.append(img_path)\n",
    "    \n",
    "    print(f\"Created {len(sample_images)} synthetic sample images\")\n",
    "\n",
    "print(f\"\\nTotal sample images available: {len(sample_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ad990",
   "metadata": {},
   "source": [
    "## 4. Single Image Grad-CAM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_image_detailed(image_path, gradcam_viz, save_prefix=None):\n",
    "    \"\"\"Detailed analysis of a single image with Grad-CAM\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        original_image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = gradcam_viz.preprocess(original_image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            prediction = torch.sigmoid(output).item()\n",
    "        \n",
    "        print(f\"\\nAnalyzing: {os.path.basename(image_path)}\")\n",
    "        print(f\"Prediction: {prediction:.4f} ({'Cancer' if prediction > 0.5 else 'No Cancer'})\")\n",
    "        print(f\"Confidence: {max(prediction, 1-prediction):.4f}\")\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        heatmap = gradcam_viz.gradcam.generate_gradcam(input_tensor)\n",
    "        \n",
    "        # Analyze attention patterns\n",
    "        analysis = gradcam_viz.analyze_attention_patterns(heatmap)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(original_image)\n",
    "        axes[0, 0].set_title(f'Original Image\\n{os.path.basename(image_path)}')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Grad-CAM heatmap\n",
    "        im = axes[0, 1].imshow(heatmap, cmap='jet')\n",
    "        axes[0, 1].set_title('Grad-CAM Heatmap')\n",
    "        axes[0, 1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[0, 1])\n",
    "        \n",
    "        # Superimposed image\n",
    "        superimposed = gradcam_viz.gradcam.superimpose_heatmap(\n",
    "            np.array(original_image), heatmap, alpha=0.4\n",
    "        )\n",
    "        axes[1, 0].imshow(superimposed)\n",
    "        axes[1, 0].set_title(f'Superimposed\\nPrediction: {prediction:.3f}')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Analysis summary\n",
    "        analysis_text = f\"\"\"\n",
    "Prediction Analysis:\n",
    "Probability: {prediction:.4f}\n",
    "Class: {'Cancer' if prediction > 0.5 else 'No Cancer'}\n",
    "Confidence: {max(prediction, 1-prediction):.4f}\n",
    "\n",
    "Attention Statistics:\n",
    "Max Activation: {analysis['max_activation']:.3f}\n",
    "Mean Activation: {analysis['mean_activation']:.3f}\n",
    "Focus Percentage: {analysis['focus_percentage']:.1f}%\n",
    "Attention Spread: {analysis['attention_spread']:.3f}\n",
    "\n",
    "Clinical Notes:\n",
    "- {'High' if prediction > 0.7 else 'Moderate' if prediction > 0.3 else 'Low'} confidence prediction\n",
    "- {'Concentrated' if analysis['focus_percentage'] > 15 else 'Distributed'} attention pattern\n",
    "- Model focus on {'specific regions' if analysis['attention_spread'] < 0.3 else 'broad areas'}\n",
    "        \"\"\"\n",
    "        \n",
    "        axes[1, 1].text(0.05, 0.95, analysis_text.strip(), transform=axes[1, 1].transAxes,\n",
    "                        verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        axes[1, 1].set_xlim(0, 1)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].axis('off')\n",
    "        axes[1, 1].set_title('Analysis Summary')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save if requested\n",
    "        if save_prefix:\n",
    "            save_path = os.path.join(RESULTS_DIR, f'{save_prefix}_{os.path.basename(image_path)}.png')\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved analysis to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'prediction': prediction,\n",
    "            'heatmap': heatmap,\n",
    "            'analysis': analysis\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {os.path.basename(image_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze first sample image\n",
    "if sample_images and gradcam_working:\n",
    "    result = analyze_single_image_detailed(sample_images[0], gradcam_viz, save_prefix='single_analysis')\n",
    "else:\n",
    "    print(\"Cannot perform single image analysis - no images or Grad-CAM not working\")\n",
    "    result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec78015",
   "metadata": {},
   "source": [
    "## 5. Batch Grad-CAM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa14294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_batch(image_paths, gradcam_viz, max_images=6):\n",
    "    \"\"\"Analyze multiple images with Grad-CAM\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    valid_images = []\n",
    "    \n",
    "    print(f\"Analyzing batch of {min(len(image_paths), max_images)} images...\")\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths[:max_images]):\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            original_image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = gradcam_viz.preprocess(original_image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                prediction = torch.sigmoid(output).item()\n",
    "            \n",
    "            # Generate Grad-CAM\n",
    "            heatmap = gradcam_viz.gradcam.generate_gradcam(input_tensor)\n",
    "            \n",
    "            results.append({\n",
    "                'image_path': image_path,\n",
    "                'image': original_image,\n",
    "                'prediction': prediction,\n",
    "                'heatmap': heatmap\n",
    "            })\n",
    "            valid_images.append(image_path)\n",
    "            \n",
    "            print(f\"  ✓ {os.path.basename(image_path)}: {prediction:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing {os.path.basename(image_path)}: {e}\")\n",
    "    \n",
    "    # Create visualization grid\n",
    "    if results:\n",
    "        n_images = len(results)\n",
    "        cols = min(3, n_images)\n",
    "        rows = (n_images + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows * 3, cols, figsize=(5*cols, 4*rows*3))\n",
    "        if n_images == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(3, -1)\n",
    "        \n",
    "        for idx, result in enumerate(results):\n",
    "            col = idx % cols\n",
    "            \n",
    "            # Original image\n",
    "            axes[0, col].imshow(result['image'])\n",
    "            axes[0, col].set_title(f\"Original\\n{os.path.basename(result['image_path'])}\")\n",
    "            axes[0, col].axis('off')\n",
    "            \n",
    "            # Grad-CAM heatmap\n",
    "            im = axes[1, col].imshow(result['heatmap'], cmap='jet')\n",
    "            axes[1, col].set_title('Grad-CAM')\n",
    "            axes[1, col].axis('off')\n",
    "            \n",
    "            # Superimposed\n",
    "            superimposed = gradcam_viz.gradcam.superimpose_heatmap(\n",
    "                np.array(result['image']), result['heatmap'], alpha=0.4\n",
    "            )\n",
    "            axes[2, col].imshow(superimposed)\n",
    "            axes[2, col].set_title(f'Overlay\\nPred: {result[\"prediction\"]:.3f}')\n",
    "            axes[2, col].axis('off')\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for idx in range(n_images, rows * cols):\n",
    "            col = idx % cols\n",
    "            for row_offset in range(3):\n",
    "                axes[row_offset, col].remove()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Batch Grad-CAM Analysis', fontsize=16, y=1.02)\n",
    "        \n",
    "        # Save batch analysis\n",
    "        save_path = os.path.join(RESULTS_DIR, 'batch_gradcam_analysis.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nBatch analysis saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze batch of images\n",
    "if sample_images and gradcam_working and len(sample_images) > 1:\n",
    "    batch_results = analyze_image_batch(sample_images, gradcam_viz, max_images=6)\n",
    "else:\n",
    "    print(\"Cannot perform batch analysis - insufficient images or Grad-CAM not working\")\n",
    "    batch_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4702a2",
   "metadata": {},
   "source": [
    "## 6. Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_patterns(results):\n",
    "    \"\"\"Analyze attention patterns across multiple images\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results available for attention analysis\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Analyzing attention patterns across {len(results)} images...\")\n",
    "    \n",
    "    # Extract attention statistics\n",
    "    predictions = [r['prediction'] for r in results]\n",
    "    heatmaps = [r['heatmap'] for r in results]\n",
    "    \n",
    "    # Calculate attention metrics for each image\n",
    "    attention_stats = []\n",
    "    for i, (pred, heatmap) in enumerate(zip(predictions, heatmaps)):\n",
    "        stats = {\n",
    "            'image_idx': i,\n",
    "            'prediction': pred,\n",
    "            'max_attention': np.max(heatmap),\n",
    "            'mean_attention': np.mean(heatmap),\n",
    "            'std_attention': np.std(heatmap),\n",
    "            'focused_pixels': np.sum(heatmap > np.percentile(heatmap.flatten(), 90)),\n",
    "            'focus_percentage': np.sum(heatmap > np.percentile(heatmap.flatten(), 90)) / heatmap.size * 100\n",
    "        }\n",
    "        attention_stats.append(stats)\n",
    "    \n",
    "    # Create comprehensive analysis plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Prediction vs Max Attention\n",
    "    preds = [s['prediction'] for s in attention_stats]\n",
    "    max_attns = [s['max_attention'] for s in attention_stats]\n",
    "    axes[0, 0].scatter(preds, max_attns, alpha=0.7, s=100)\n",
    "    axes[0, 0].set_xlabel('Prediction Probability')\n",
    "    axes[0, 0].set_ylabel('Max Attention')\n",
    "    axes[0, 0].set_title('Prediction vs Max Attention')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    if len(preds) > 1:\n",
    "        corr = np.corrcoef(preds, max_attns)[0, 1]\n",
    "        axes[0, 0].text(0.05, 0.95, f'Corr: {corr:.3f}', transform=axes[0, 0].transAxes, \n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Prediction vs Mean Attention\n",
    "    mean_attns = [s['mean_attention'] for s in attention_stats]\n",
    "    axes[0, 1].scatter(preds, mean_attns, alpha=0.7, s=100, color='orange')\n",
    "    axes[0, 1].set_xlabel('Prediction Probability')\n",
    "    axes[0, 1].set_ylabel('Mean Attention')\n",
    "    axes[0, 1].set_title('Prediction vs Mean Attention')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(preds) > 1:\n",
    "        corr = np.corrcoef(preds, mean_attns)[0, 1]\n",
    "        axes[0, 1].text(0.05, 0.95, f'Corr: {corr:.3f}', transform=axes[0, 1].transAxes,\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 3. Focus Percentage vs Prediction\n",
    "    focus_pcts = [s['focus_percentage'] for s in attention_stats]\n",
    "    axes[0, 2].scatter(preds, focus_pcts, alpha=0.7, s=100, color='green')\n",
    "    axes[0, 2].set_xlabel('Prediction Probability')\n",
    "    axes[0, 2].set_ylabel('Focus Percentage (%)')\n",
    "    axes[0, 2].set_title('Prediction vs Focus Concentration')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(preds) > 1:\n",
    "        corr = np.corrcoef(preds, focus_pcts)[0, 1]\n",
    "        axes[0, 2].text(0.05, 0.95, f'Corr: {corr:.3f}', transform=axes[0, 2].transAxes,\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 4. Attention Distribution\n",
    "    all_attentions = np.concatenate([h.flatten() for h in heatmaps])\n",
    "    axes[1, 0].hist(all_attentions, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Attention Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Overall Attention Distribution')\n",
    "    axes[1, 0].axvline(np.mean(all_attentions), color='red', linestyle='--', label='Mean')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 5. Attention Variability\n",
    "    std_attns = [s['std_attention'] for s in attention_stats]\n",
    "    axes[1, 1].scatter(mean_attns, std_attns, alpha=0.7, s=100, color='red')\n",
    "    axes[1, 1].set_xlabel('Mean Attention')\n",
    "    axes[1, 1].set_ylabel('Attention Std Dev')\n",
    "    axes[1, 1].set_title('Attention Mean vs Variability')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Summary Statistics\n",
    "    stats_text = f\"\"\"\n",
    "Attention Analysis Summary:\n",
    "\n",
    "Images analyzed: {len(results)}\n",
    "\n",
    "Prediction Range:\n",
    "  Min: {min(preds):.3f}\n",
    "  Max: {max(preds):.3f}\n",
    "  Mean: {np.mean(preds):.3f}\n",
    "\n",
    "Attention Statistics:\n",
    "  Mean Max Attention: {np.mean(max_attns):.3f}\n",
    "  Mean Focus %: {np.mean(focus_pcts):.1f}%\n",
    "  \n",
    "High Confidence Predictions: {sum(1 for p in preds if abs(p-0.5) > 0.3)}\n",
    "Low Confidence Predictions: {sum(1 for p in preds if abs(p-0.5) <= 0.3)}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.95, stats_text.strip(), transform=axes[1, 2].transAxes, \n",
    "                    verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[1, 2].set_xlim(0, 1)\n",
    "    axes[1, 2].set_ylim(0, 1)\n",
    "    axes[1, 2].axis('off')\n",
    "    axes[1, 2].set_title('Summary Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Attention Pattern Analysis', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Save analysis\n",
    "    save_path = os.path.join(RESULTS_DIR, 'attention_pattern_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Attention analysis saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return attention_stats\n",
    "\n",
    "# Perform attention analysis\n",
    "if batch_results:\n",
    "    attention_stats = analyze_attention_patterns(batch_results)\n",
    "else:\n",
    "    print(\"No batch results available for attention pattern analysis\")\n",
    "    attention_stats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9c5d0",
   "metadata": {},
   "source": [
    "## 7. Clinical Interpretation Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clinical_interpretation(prediction, attention_stats):\n",
    "    \"\"\"Generate clinical interpretation based on prediction and attention patterns\"\"\"\n",
    "    \n",
    "    interpretation = []\n",
    "    recommendations = []\n",
    "    confidence_level = \"Unknown\"\n",
    "    \n",
    "    # Prediction interpretation\n",
    "    if prediction >= 0.8:\n",
    "        interpretation.append(\"HIGH probability of malignant findings detected.\")\n",
    "        confidence_level = \"High\"\n",
    "        recommendations.extend([\n",
    "            \"Immediate radiologist review recommended\",\n",
    "            \"Consider additional imaging (CT scan) if clinically indicated\",\n",
    "            \"Follow-up with oncology consultation\"\n",
    "        ])\n",
    "    elif prediction >= 0.6:\n",
    "        interpretation.append(\"MODERATE probability of suspicious findings.\")\n",
    "        confidence_level = \"Moderate\"\n",
    "        recommendations.extend([\n",
    "            \"Radiologist review within 24-48 hours\",\n",
    "            \"Consider repeat imaging in 3-6 months\",\n",
    "            \"Correlate with clinical symptoms\"\n",
    "        ])\n",
    "    elif prediction >= 0.4:\n",
    "        interpretation.append(\"LOW to MODERATE probability of abnormal findings.\")\n",
    "        confidence_level = \"Moderate\"\n",
    "        recommendations.extend([\n",
    "            \"Routine radiologist review\",\n",
    "            \"Standard follow-up protocols\",\n",
    "            \"Consider patient history and symptoms\"\n",
    "        ])\n",
    "    else:\n",
    "        interpretation.append(\"LOW probability of malignant findings.\")\n",
    "        confidence_level = \"High\" if prediction < 0.2 else \"Moderate\"\n",
    "        recommendations.extend([\n",
    "            \"Routine radiologist review\",\n",
    "            \"Standard follow-up protocols\",\n",
    "            \"Continue regular screening as appropriate\"\n",
    "        ])\n",
    "    \n",
    "    # Attention pattern interpretation\n",
    "    if attention_stats:\n",
    "        focus_pct = attention_stats.get('focus_percentage', 0)\n",
    "        max_attention = attention_stats.get('max_attention', 0)\n",
    "        \n",
    "        if focus_pct > 15:\n",
    "            interpretation.append(\"Model attention is HIGHLY CONCENTRATED on specific regions.\")\n",
    "            interpretation.append(\"This suggests the presence of localized abnormalities.\")\n",
    "        elif focus_pct > 5:\n",
    "            interpretation.append(\"Model attention is MODERATELY FOCUSED on specific regions.\")\n",
    "        else:\n",
    "            interpretation.append(\"Model attention is DIFFUSE across the image.\")\n",
    "            interpretation.append(\"No specific regions of high concern identified.\")\n",
    "        \n",
    "        if max_attention > 0.8:\n",
    "            interpretation.append(\"Very strong activation detected in attention regions.\")\n",
    "        elif max_attention > 0.5:\n",
    "            interpretation.append(\"Moderate activation detected in attention regions.\")\n",
    "    \n",
    "    # Model confidence assessment\n",
    "    model_confidence = max(prediction, 1 - prediction)\n",
    "    if model_confidence < 0.7:\n",
    "        recommendations.append(\"LOW MODEL CONFIDENCE - Prioritize human expert review\")\n",
    "        interpretation.append(\"Model shows uncertainty in this case.\")\n",
    "    \n",
    "    # Always add disclaimer\n",
    "    recommendations.append(\"IMPORTANT: This AI system is a diagnostic aid only - clinical judgment should always take precedence\")\n",
    "    \n",
    "    return {\n",
    "        'interpretation': interpretation,\n",
    "        'recommendations': recommendations,\n",
    "        'confidence_level': confidence_level,\n",
    "        'model_confidence': model_confidence\n",
    "    }\n",
    "\n",
    "def display_clinical_report(image_path, prediction, attention_stats=None):\n",
    "    \"\"\"Display a clinical interpretation report\"\"\"\n",
    "    \n",
    "    clinical_info = generate_clinical_interpretation(prediction, attention_stats)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"                    CLINICAL INTERPRETATION REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Model Architecture: {config['model']['architecture']}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(\"PREDICTION RESULTS:\")\n",
    "    print(f\"  Probability Score: {prediction:.4f}\")\n",
    "    print(f\"  Binary Prediction: {'CANCER' if prediction > 0.5 else 'NO CANCER'}\")\n",
    "    print(f\"  Model Confidence: {clinical_info['model_confidence']:.4f} ({clinical_info['confidence_level']})\")\n",
    "    \n",
    "    if attention_stats:\n",
    "        print(\"\\nATTENTION ANALYSIS:\")\n",
    "        print(f\"  Focus Percentage: {attention_stats.get('focus_percentage', 0):.1f}%\")\n",
    "        print(f\"  Max Attention: {attention_stats.get('max_attention', 0):.3f}\")\n",
    "        print(f\"  Mean Attention: {attention_stats.get('mean_attention', 0):.3f}\")\n",
    "    \n",
    "    print(\"\\nCLINICAL INTERPRETATION:\")\n",
    "    for i, interp in enumerate(clinical_info['interpretation'], 1):\n",
    "        print(f\"  {i}. {interp}\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(clinical_info['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return clinical_info\n",
    "\n",
    "# Generate clinical report for first analyzed image\n",
    "if batch_results:\n",
    "    first_result = batch_results[0]\n",
    "    first_attention = attention_stats[0] if attention_stats else None\n",
    "    \n",
    "    clinical_report = display_clinical_report(\n",
    "        first_result['image_path'],\n",
    "        first_result['prediction'],\n",
    "        first_attention\n",
    "    )\n",
    "else:\n",
    "    print(\"No results available for clinical interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ea348",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grad-CAM Visualization Analysis Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if gradcam_working:\n",
    "    print(\"✓ Grad-CAM successfully implemented and tested\")\n",
    "    \n",
    "    if batch_results:\n",
    "        n_analyzed = len(batch_results)\n",
    "        predictions = [r['prediction'] for r in batch_results]\n",
    "        \n",
    "        print(f\"✓ Successfully analyzed {n_analyzed} images\")\n",
    "        print(f\"  - Prediction range: [{min(predictions):.3f}, {max(predictions):.3f}]\")\n",
    "        print(f\"  - Mean prediction: {np.mean(predictions):.3f}\")\n",
    "        print(f\"  - Cancer predictions (>0.5): {sum(1 for p in predictions if p > 0.5)}\")\n",
    "        \n",
    "        if attention_stats:\n",
    "            focus_percentages = [s['focus_percentage'] for s in attention_stats]\n",
    "            print(f\"  - Mean focus percentage: {np.mean(focus_percentages):.1f}%\")\n",
    "            print(f\"  - Highly focused images (>15%): {sum(1 for f in focus_percentages if f > 15)}\")\n",
    "else:\n",
    "    print(\"✗ Grad-CAM implementation encountered issues\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "insights = [\n",
    "    \"• Grad-CAM provides valuable interpretability for cancer detection models\",\n",
    "    \"• Attention patterns vary significantly across different images\",\n",
    "    \"• Higher prediction confidence often correlates with focused attention\",\n",
    "    \"• Clinical interpretation requires combining prediction scores with attention analysis\",\n",
    "    \"• Model explanations can aid radiologists in understanding AI decisions\",\n",
    "    \"• Visual explanations help identify potential biases or artifacts\",\n",
    "    \"• Different architectures may show different attention patterns\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(f\"\\nAnalysis complete! All visualizations and reports saved to: {RESULTS_DIR}\")\n",
    "\n",
    "if not trained_model_available:\n",
    "    print(\"\\nNote: Analysis was performed with a randomly initialized model.\")\n",
    "    print(\"For meaningful clinical insights, please train the model first using the baseline notebook.\")\n",
    "\n",
    "print(\"\\nRecommendations for Further Analysis:\")\n",
    "print(\"- Train models with larger datasets for better clinical relevance\")\n",
    "print(\"- Compare attention patterns across different model architectures\") \n",
    "print(\"- Validate Grad-CAM interpretations with radiologist annotations\")\n",
    "print(\"- Analyze failure cases to identify model limitations\")\n",
    "print(\"- Implement additional explainability techniques (LIME, SHAP, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fee84e",
   "metadata": {},
   "source": [
    "## 9. Error Case Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_errors(batch_results, threshold=0.5):\n",
    "    \"\"\"Analyze cases where model predictions might be questionable\"\"\"\n",
    "    \n",
    "    if not batch_results:\n",
    "        print(\"No batch results available for error analysis\")\n",
    "        return []\n",
    "    \n",
    "    print(\"Analyzing Potential Prediction Errors...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Categorize predictions\n",
    "    high_confidence_cancer = []\n",
    "    high_confidence_normal = []\n",
    "    low_confidence = []\n",
    "    \n",
    "    for result in batch_results:\n",
    "        pred = result['prediction']\n",
    "        confidence = max(pred, 1 - pred)\n",
    "        \n",
    "        if pred > 0.7:\n",
    "            high_confidence_cancer.append(result)\n",
    "        elif pred < 0.3:\n",
    "            high_confidence_normal.append(result)\n",
    "        else:\n",
    "            low_confidence.append(result)\n",
    "    \n",
    "    print(f\"High Confidence Cancer (>0.7): {len(high_confidence_cancer)} cases\")\n",
    "    print(f\"High Confidence Normal (<0.3): {len(high_confidence_normal)} cases\")\n",
    "    print(f\"Low Confidence (0.3-0.7): {len(low_confidence)} cases\")\n",
    "    \n",
    "    # Analyze low confidence cases in detail\n",
    "    if low_confidence:\n",
    "        print(f\"\\nAnalyzing {len(low_confidence)} Low Confidence Cases:\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, min(3, len(low_confidence)), figsize=(5*min(3, len(low_confidence)), 8))\n",
    "        if len(low_confidence) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        elif min(3, len(low_confidence)) == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        for idx, result in enumerate(low_confidence[:3]):\n",
    "            col = idx\n",
    "            pred = result['prediction']\n",
    "            \n",
    "            print(f\"  Case {idx+1}: {os.path.basename(result['image_path'])}\")\n",
    "            print(f\"    Prediction: {pred:.4f} (Confidence: {max(pred, 1-pred):.4f})\")\n",
    "            \n",
    "            # Original image\n",
    "            axes[0, col].imshow(result['image'])\n",
    "            axes[0, col].set_title(f\"Low Confidence Case {idx+1}\\nPred: {pred:.3f}\")\n",
    "            axes[0, col].axis('off')\n",
    "            \n",
    "            # Grad-CAM overlay\n",
    "            superimposed = gradcam_viz.gradcam.superimpose_heatmap(\n",
    "                np.array(result['image']), result['heatmap'], alpha=0.4\n",
    "            )\n",
    "            axes[1, col].imshow(superimposed)\n",
    "            axes[1, col].set_title(f\"Attention Pattern\\nUncertain: {abs(pred-0.5):.3f}\")\n",
    "            axes[1, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Low Confidence Prediction Analysis', fontsize=14, y=1.02)\n",
    "        \n",
    "        # Save error analysis\n",
    "        save_path = os.path.join(RESULTS_DIR, 'error_case_analysis.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nError case analysis saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'high_confidence_cancer': high_confidence_cancer,\n",
    "        'high_confidence_normal': high_confidence_normal,\n",
    "        'low_confidence': low_confidence\n",
    "    }\n",
    "\n",
    "# Perform error case analysis\n",
    "if batch_results:\n",
    "    error_analysis = analyze_prediction_errors(batch_results)\n",
    "else:\n",
    "    print(\"No batch results available for error analysis\")\n",
    "    error_analysis = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ab919",
   "metadata": {},
   "source": [
    "## 10. Multi-Model Comparison (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7747150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_gradcam(image_path, model_configs, device):\n",
    "    \"\"\"Compare Grad-CAM visualizations across different model architectures\"\"\"\n",
    "    \n",
    "    print(f\"Comparing models on: {os.path.basename(image_path)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Load and preprocess image once\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Test each model architecture\n",
    "    for model_name, config in model_configs.items():\n",
    "        try:\n",
    "            # Load model\n",
    "            model = get_model({'model': config})\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Initialize Grad-CAM\n",
    "            gradcam_viz = GradCAMVisualizer(model, device)\n",
    "            input_tensor = gradcam_viz.preprocess(original_image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                prediction = torch.sigmoid(output).item()\n",
    "            \n",
    "            # Generate Grad-CAM\n",
    "            heatmap = gradcam_viz.gradcam.generate_gradcam(input_tensor)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'prediction': prediction,\n",
    "                'heatmap': heatmap,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ {model_name}: Prediction = {prediction:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Visualize comparison\n",
    "    if len(results) > 1:\n",
    "        n_models = len(results)\n",
    "        fig, axes = plt.subplots(3, n_models, figsize=(4*n_models, 12))\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        model_names = list(results.keys())\n",
    "        \n",
    "        for idx, model_name in enumerate(model_names):\n",
    "            result = results[model_name]\n",
    "            \n",
    "            # Original image (show once)\n",
    "            if idx == 0:\n",
    "                axes[0, idx].imshow(original_image)\n",
    "                axes[0, idx].set_title(f'Original Image\\n{os.path.basename(image_path)}')\n",
    "            else:\n",
    "                axes[0, idx].imshow(original_image)\n",
    "                axes[0, idx].set_title('Original Image')\n",
    "            axes[0, idx].axis('off')\n",
    "            \n",
    "            # Grad-CAM heatmap\n",
    "            im = axes[1, idx].imshow(result['heatmap'], cmap='jet')\n",
    "            axes[1, idx].set_title(f'{model_name}\\nGrad-CAM')\n",
    "            axes[1, idx].axis('off')\n",
    "            \n",
    "            # Superimposed\n",
    "            superimposed = gradcam_viz.gradcam.superimpose_heatmap(\n",
    "                np.array(original_image), result['heatmap'], alpha=0.4\n",
    "            )\n",
    "            axes[2, idx].imshow(superimposed)\n",
    "            axes[2, idx].set_title(f'{model_name}\\nPred: {result[\"prediction\"]:.3f}')\n",
    "            axes[2, idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Multi-Model Grad-CAM Comparison', fontsize=16, y=1.02)\n",
    "        \n",
    "        # Save comparison\n",
    "        save_path = os.path.join(RESULTS_DIR, 'multi_model_comparison.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nMulti-model comparison saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Print comparison summary\n",
    "        print(\"\\nModel Comparison Summary:\")\n",
    "        print(\"-\" * 30)\n",
    "        for model_name, result in results.items():\n",
    "            pred = result['prediction']\n",
    "            confidence = max(pred, 1 - pred)\n",
    "            decision = \"Cancer\" if pred > 0.5 else \"Normal\"\n",
    "            print(f\"{model_name:12}: {pred:.4f} ({decision}, Conf: {confidence:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example model comparison (uncomment and modify as needed)\n",
    "# model_comparison_configs = {\n",
    "#     'ResNet18': {\n",
    "#         'architecture': 'resnet18',\n",
    "#         'num_classes': 1,\n",
    "#         'pretrained': True,\n",
    "#         'dropout': 0.5\n",
    "#     },\n",
    "#     'ResNet50': {\n",
    "#         'architecture': 'resnet50', \n",
    "#         'num_classes': 1,\n",
    "#         'pretrained': True,\n",
    "#         'dropout': 0.5\n",
    "#     },\n",
    "#     'Custom CNN': {\n",
    "#         'architecture': 'custom_cnn',\n",
    "#         'num_classes': 1,\n",
    "#         'pretrained': False,\n",
    "#         'dropout': 0.5\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# if sample_images:\n",
    "#     model_comparison = compare_models_gradcam(sample_images[0], model_comparison_configs, device)\n",
    "\n",
    "print(\"Multi-model comparison function defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eef6f5",
   "metadata": {},
   "source": [
    "## 11. Advanced Visualization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_attention(model, image_tensor, target_layers=None):\n",
    "    \"\"\"Analyze attention patterns across different layers\"\"\"\n",
    "    \n",
    "    print(\"Analyzing Layer-wise Attention Patterns...\")\n",
    "    \n",
    "    if target_layers is None:\n",
    "        # Default layers for ResNet\n",
    "        if hasattr(model, 'layer1'):\n",
    "            target_layers = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "        else:\n",
    "            print(\"Custom target layers needed for this architecture\")\n",
    "            return None\n",
    "    \n",
    "    layer_outputs = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            layer_outputs[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    for layer_name in target_layers:\n",
    "        if hasattr(model, layer_name):\n",
    "            layer = getattr(model, layer_name)\n",
    "            hooks.append(layer.register_forward_hook(get_activation(layer_name)))\n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            _ = model(image_tensor)\n",
    "        \n",
    "        # Analyze each layer\n",
    "        fig, axes = plt.subplots(2, len(target_layers), figsize=(4*len(target_layers), 8))\n",
    "        if len(target_layers) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for idx, layer_name in enumerate(target_layers):\n",
    "            if layer_name in layer_outputs:\n",
    "                activation = layer_outputs[layer_name]\n",
    "                \n",
    "                # Average across channels and resize\n",
    "                avg_activation = torch.mean(activation, dim=1).squeeze().cpu().numpy()\n",
    "                resized = cv2.resize(avg_activation, (224, 224))\n",
    "                \n",
    "                # Normalize\n",
    "                resized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)\n",
    "                \n",
    "                # Heatmap\n",
    "                axes[0, idx].imshow(resized, cmap='jet')\n",
    "                axes[0, idx].set_title(f'{layer_name}\\nActivation Map')\n",
    "                axes[0, idx].axis('off')\n",
    "                \n",
    "                # Histogram of activations\n",
    "                axes[1, idx].hist(avg_activation.flatten(), bins=50, alpha=0.7)\n",
    "                axes[1, idx].set_title(f'{layer_name}\\nActivation Distribution')\n",
    "                axes[1, idx].set_xlabel('Activation Value')\n",
    "                axes[1, idx].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Layer-wise Attention Analysis', fontsize=14, y=1.02)\n",
    "        \n",
    "        # Save layer analysis\n",
    "        save_path = os.path.join(RESULTS_DIR, 'layer_attention_analysis.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Layer analysis saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    finally:\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "    \n",
    "    return layer_outputs\n",
    "\n",
    "# Perform layer-wise analysis if we have results\n",
    "if sample_images and gradcam_working:\n",
    "    # Use first sample image\n",
    "    original_image = Image.open(sample_images[0]).convert('RGB')\n",
    "    input_tensor = gradcam_viz.preprocess(original_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    layer_analysis = analyze_layer_attention(model, input_tensor)\n",
    "else:\n",
    "    print(\"Cannot perform layer analysis - no images or model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradcam_metrics(batch_results, attention_stats):\n",
    "    \"\"\"Calculate quantitative metrics for Grad-CAM quality assessment\"\"\"\n",
    "    \n",
    "    if not batch_results or not attention_stats:\n",
    "        print(\"Insufficient data for quantitative evaluation\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Calculating Grad-CAM Quality Metrics...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    metrics = {\n",
    "        'focus_consistency': [],\n",
    "        'prediction_attention_correlation': [],\n",
    "        'attention_entropy': [],\n",
    "        'spatial_coherence': []\n",
    "    }\n",
    "    \n",
    "    predictions = [r['prediction'] for r in batch_results]\n",
    "    focus_percentages = [s['focus_percentage'] for s in attention_stats]\n",
    "    max_attentions = [s['max_attention'] for s in attention_stats]\n",
    "    \n",
    "    # 1. Focus Consistency (std of focus percentages)\n",
    "    focus_std = np.std(focus_percentages)\n",
    "    metrics['focus_consistency'] = focus_std\n",
    "    \n",
    "    # 2. Prediction-Attention Correlation\n",
    "    if len(predictions) > 1:\n",
    "        corr_max = np.corrcoef(predictions, max_attentions)[0, 1]\n",
    "        corr_focus = np.corrcoef(predictions, focus_percentages)[0, 1]\n",
    "        metrics['prediction_attention_correlation'] = {\n",
    "            'max_attention': corr_max,\n",
    "            'focus_percentage': corr_focus\n",
    "        }\n",
    "    \n",
    "    # 3. Attention Entropy (measure of attention spread)\n",
    "    entropies = []\n",
    "    for result in batch_results:\n",
    "        heatmap = result['heatmap']\n",
    "        # Normalize to probability distribution\n",
    "        heatmap_norm = heatmap / (np.sum(heatmap) + 1e-8)\n",
    "        # Calculate entropy\n",
    "        entropy = -np.sum(heatmap_norm * np.log(heatmap_norm + 1e-8))\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    metrics['attention_entropy'] = {\n",
    "        'mean': np.mean(entropies),\n",
    "        'std': np.std(entropies),\n",
    "        'individual': entropies\n",
    "    }\n",
    "    \n",
    "    # 4. Spatial Coherence (measure of attention clustering)\n",
    "    coherence_scores = []\n",
    "    for result in batch_results:\n",
    "        heatmap = result['heatmap']\n",
    "        # Apply Gaussian filter and measure similarity\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        smoothed = gaussian_filter(heatmap, sigma=2.0)\n",
    "        coherence = np.corrcoef(heatmap.flatten(), smoothed.flatten())[0, 1]\n",
    "        coherence_scores.append(coherence)\n",
    "    \n",
    "    metrics['spatial_coherence'] = {\n",
    "        'mean': np.mean(coherence_scores),\n",
    "        'std': np.std(coherence_scores),\n",
    "        'individual': coherence_scores\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Grad-CAM Quality Assessment:\")\n",
    "    print(f\"Focus Consistency (lower=more consistent): {focus_std:.3f}\")\n",
    "    \n",
    "    if 'max_attention' in metrics['prediction_attention_correlation']:\n",
    "        print(f\"Prediction-Max Attention Correlation: {metrics['prediction_attention_correlation']['max_attention']:.3f}\")\n",
    "        print(f\"Prediction-Focus Correlation: {metrics['prediction_attention_correlation']['focus_percentage']:.3f}\")\n",
    "    \n",
    "    print(f\"Mean Attention Entropy: {metrics['attention_entropy']['mean']:.3f} ± {metrics['attention_entropy']['std']:.3f}\")\n",
    "    print(f\"Mean Spatial Coherence: {metrics['spatial_coherence']['mean']:.3f} ± {metrics['spatial_coherence']['std']:.3f}\")\n",
    "    \n",
    "    # Visualize metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Entropy distribution\n",
    "    axes[0, 0].hist(entropies, bins=20, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Attention Entropy Distribution')\n",
    "    axes[0, 0].set_xlabel('Entropy')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(entropies), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Spatial coherence distribution\n",
    "    axes[0, 1].hist(coherence_scores, bins=20, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Spatial Coherence Distribution')\n",
    "    axes[0, 1].set_xlabel('Coherence Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].axvline(np.mean(coherence_scores), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Prediction vs Entropy\n",
    "    axes[1, 0].scatter(predictions, entropies, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Prediction Probability')\n",
    "    axes[1, 0].set_ylabel('Attention Entropy')\n",
    "    axes[1, 0].set_title('Prediction vs Attention Entropy')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction vs Coherence\n",
    "    axes[1, 1].scatter(predictions, coherence_scores, alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_xlabel('Prediction Probability')\n",
    "    axes[1, 1].set_ylabel('Spatial Coherence')\n",
    "    axes[1, 1].set_title('Prediction vs Spatial Coherence')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Grad-CAM Quality Metrics', fontsize=14, y=1.02)\n",
    "    \n",
    "    # Save metrics visualization\n",
    "    save_path = os.path.join(RESULTS_DIR, 'gradcam_quality_metrics.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nMetrics visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "if batch_results and attention_stats:\n",
    "    gradcam_metrics = calculate_gradcam_metrics(batch_results, attention_stats)\n",
    "else:\n",
    "    print(\"Cannot calculate metrics - insufficient data\")\n",
    "    gradcam_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c97218",
   "metadata": {},
   "source": [
    "## 12. Export Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(batch_results, attention_stats, gradcam_metrics, output_path):\n",
    "    \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "    \n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    report = {\n",
    "        'analysis_metadata': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_architecture': config['model']['architecture'],\n",
    "            'total_images_analyzed': len(batch_results) if batch_results else 0,\n",
    "            'gradcam_working': gradcam_working,\n",
    "            'trained_model_available': trained_model_available\n",
    "        },\n",
    "        'image_analysis': [],\n",
    "        'summary_statistics': {},\n",
    "        'quality_metrics': gradcam_metrics,\n",
    "        'clinical_insights': []\n",
    "    }\n",
    "    \n",
    "    # Individual image analysis\n",
    "    if batch_results:\n",
    "        for idx, result in enumerate(batch_results):\n",
    "            image_analysis = {\n",
    "                'image_path': os.path.basename(result['image_path']),\n",
    "                'prediction': float(result['prediction']),\n",
    "                'prediction_class': 'Cancer' if result['prediction'] > 0.5 else 'Normal',\n",
    "                'confidence': float(max(result['prediction'], 1 - result['prediction']))\n",
    "            }\n",
    "            \n",
    "            if attention_stats and idx < len(attention_stats):\n",
    "                stats = attention_stats[idx]\n",
    "                image_analysis.update({\n",
    "                    'max_attention': float(stats['max_attention']),\n",
    "                    'mean_attention': float(stats['mean_attention']),\n",
    "                    'focus_percentage': float(stats['focus_percentage']),\n",
    "                    'attention_spread': float(stats['std_attention'])\n",
    "                })\n",
    "            \n",
    "            report['image_analysis'].append(image_analysis)\n",
    "    \n",
    "    # Summary statistics\n",
    "    if batch_results:\n",
    "        predictions = [r['prediction'] for r in batch_results]\n",
    "        report['summary_statistics'] = {\n",
    "            'prediction_range': [float(min(predictions)), float(max(predictions))],\n",
    "            'mean_prediction': float(np.mean(predictions)),\n",
    "            'cancer_predictions': int(sum(1 for p in predictions if p > 0.5)),\n",
    "            'normal_predictions': int(sum(1 for p in predictions if p <= 0.5)),\n",
    "            'high_confidence_predictions': int(sum(1 for p in predictions if abs(p-0.5) > 0.3)),\n",
    "            'low_confidence_predictions': int(sum(1 for p in predictions if abs(p-0.5) <= 0.3))\n",
    "        }\n",
    "        \n",
    "        if attention_stats:\n",
    "            focus_percentages = [s['focus_percentage'] for s in attention_stats]\n",
    "            report['summary_statistics'].update({\n",
    "                'mean_focus_percentage': float(np.mean(focus_percentages)),\n",
    "                'highly_focused_images': int(sum(1 for f in focus_percentages if f > 15))\n",
    "            })\n",
    "    \n",
    "    # Clinical insights\n",
    "    report['clinical_insights'] = [\n",
    "        \"Grad-CAM visualizations provide interpretable insights into model decision-making\",\n",
    "        \"Attention patterns vary significantly across different images and prediction confidence levels\",\n",
    "        \"Higher prediction confidence often correlates with more focused attention patterns\",\n",
    "        \"Low confidence predictions require additional clinical review and validation\",\n",
    "        \"Visual explanations can help identify potential model biases or artifacts\",\n",
    "        \"Integration with clinical workflows requires validation against radiologist annotations\"\n",
    "    ]\n",
    "    \n",
    "    # Save JSON report\n",
    "    json_path = os.path.join(output_path, 'gradcam_analysis_report.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    # Generate markdown summary\n",
    "    md_path = os.path.join(output_path, 'gradcam_analysis_summary.md')\n",
    "    with open(md_path, 'w') as f:\n",
    "        f.write(\"# Grad-CAM Analysis Summary Report\\n\\n\")\n",
    "        f.write(f\"**Generated:** {report['analysis_metadata']['timestamp']}\\n\")\n",
    "        f.write(f\"**Model:** {report['analysis_metadata']['model_architecture']}\\n\")\n",
    "        f.write(f\"**Images Analyzed:** {report['analysis_metadata']['total_images_analyzed']}\\n\\n\")\n",
    "        \n",
    "        if report['summary_statistics']:\n",
    "            f.write(\"## Summary Statistics\\n\\n\")\n",
    "            stats = report['summary_statistics']\n",
    "            f.write(f\"- **Prediction Range:** {stats['prediction_range'][0]:.3f} - {stats['prediction_range'][1]:.3f}\\n\")\n",
    "            f.write(f\"- **Mean Prediction:** {stats['mean_prediction']:.3f}\\n\")\n",
    "            f.write(f\"- **Cancer Predictions:** {stats['cancer_predictions']}\\n\")\n",
    "            f.write(f\"- **Normal Predictions:** {stats['normal_predictions']}\\n\")\n",
    "            f.write(f\"- **High Confidence:** {stats['high_confidence_predictions']}\\n\")\n",
    "            f.write(f\"- **Low Confidence:** {stats['low_confidence_predictions']}\\n\")\n",
    "            \n",
    "            if 'mean_focus_percentage' in stats:\n",
    "                f.write(f\"- **Mean Focus Percentage:** {stats['mean_focus_percentage']:.1f}%\\n\")\n",
    "                f.write(f\"- **Highly Focused Images:** {stats['highly_focused_images']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Clinical Insights\\n\\n\")\n",
    "        for insight in report['clinical_insights']:\n",
    "            f.write(f\"- {insight}\\n\")\n",
    "        \n",
    "        if not trained_model_available:\n",
    "            f.write(\"\\n## Important Note\\n\\n\")\n",
    "            f.write(\"**This analysis was performed with a randomly initialized model.** \")\n",
    "            f.write(\"For clinically meaningful insights, please train the model using the baseline notebook first.\\n\")\n",
    "    \n",
    "    print(f\"Comprehensive report generated:\")\n",
    "    print(f\"- JSON Report: {json_path}\")\n",
    "    print(f\"- Markdown Summary: {md_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate comprehensive report\n",
    "if batch_results:\n",
    "    comprehensive_report = generate_comprehensive_report(\n",
    "        batch_results, attention_stats, gradcam_metrics, RESULTS_DIR\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"All results saved to: {RESULTS_DIR}\")\n",
    "    print(f\"Total files generated: {len(list(Path(RESULTS_DIR).glob('*.*')))}\")\n",
    "else:\n",
    "    print(\"No results available for comprehensive report generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424780c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏥 GRAD-CAM VISUALIZATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analysis summary\n",
    "if gradcam_working:\n",
    "    print(\"✅ Grad-CAM implementation: SUCCESS\")\n",
    "    if batch_results:\n",
    "        print(f\"✅ Images analyzed: {len(batch_results)}\")\n",
    "        predictions = [r['prediction'] for r in batch_results]\n",
    "        print(f\"✅ Prediction range: {min(predictions):.3f} - {max(predictions):.3f}\")\n",
    "        cancer_count = sum(1 for p in predictions if p > 0.5)\n",
    "        print(f\"✅ Cancer predictions: {cancer_count}/{len(predictions)}\")\n",
    "    else:\n",
    "        print(\"⚠️  No images were successfully analyzed\")\n",
    "else:\n",
    "    print(\"❌ Grad-CAM implementation: FAILED\")\n",
    "\n",
    "# Quality assessment\n",
    "if gradcam_metrics:\n",
    "    print(f\"✅ Quality metrics calculated\")\n",
    "    entropy_mean = gradcam_metrics['attention_entropy']['mean']\n",
    "    coherence_mean = gradcam_metrics['spatial_coherence']['mean']\n",
    "    print(f\"   - Attention entropy: {entropy_mean:.3f}\")\n",
    "    print(f\"   - Spatial coherence: {coherence_mean:.3f}\")\n",
    "\n",
    "# Files generated\n",
    "results_files = list(Path(RESULTS_DIR).glob('*.*'))\n",
    "print(f\"📁 Files generated: {len(results_files)}\")\n",
    "for file_path in results_files:\n",
    "    print(f\"   - {file_path.name}\")\n",
    "\n",
    "print(\"\\n🔬 CLINICAL INTERPRETATION GUIDELINES:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. High confidence predictions (>0.7 or <0.3): Prioritize for review\")\n",
    "print(\"2. Focused attention patterns (>15%): Indicate localized findings\")\n",
    "print(\"3. Low confidence predictions (0.3-0.7): Require expert validation\")\n",
    "print(\"4. Diffuse attention patterns: May indicate subtle or absent findings\")\n",
    "print(\"5. Always combine AI predictions with clinical judgment\")\n",
    "\n",
    "print(\"\\n📋 NEXT STEPS & RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "next_steps = [\n",
    "    \"Train model with real data for meaningful clinical insights\",\n",
    "    \"Validate Grad-CAM explanations against radiologist annotations\", \n",
    "    \"Implement additional explainability techniques (LIME, SHAP)\",\n",
    "    \"Conduct clinical evaluation with medical experts\",\n",
    "    \"Analyze model performance on different patient demographics\",\n",
    "    \"Integrate findings into clinical decision support system\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "if not trained_model_available:\n",
    "    print(\"\\n⚠️  IMPORTANT NOTICE:\")\n",
    "    print(\"This analysis used a randomly initialized model for demonstration.\")\n",
    "    print(\"Train the model using the baseline notebook for clinical relevance.\")\n",
    "\n",
    "print(f\"\\n📊 Complete analysis saved to: {RESULTS_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
